# Web Crawler

A Web crawler, sometimes called a spider or spiderbot and often shortened to crawler, is an Internet bot that systematically browses the World Wide Web, typically for the purpose of Web indexing (web spidering).
*(Wikipedia)*

This web crawler is coded in C language. It uses Wget linux command to download web pages.

## Compilation

Wget command is required.

Use the GNU GCC compiler to compile this program.

```bash
gcc crawler.c -o crawler
```

## Usage

```bash
./crawler <directory name> <seed URL>
```

## Contributing
Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.
